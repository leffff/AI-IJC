{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "back_translation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2636mY6VC5WB"
   },
   "source": [
    "The main purpose of this file is to back translate unlabeled comments in order to create augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7diN-4bkSU4p"
   },
   "source": [
    "#  Data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aU0IQcyzDROj"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m43uzs17nkSh"
   },
   "source": [
    "train_labeled = pd.read_csv('../Data_rus/labled_train_data.csv', index_col=0, sep=\"\\t\")\n",
    "comments_labeled = pd.read_csv('../Data_rus/labled_train_comments.csv', index_col=0, sep=\"\\t\")\n",
    "comments_unlabeled = pd.read_csv('../Data_rus/unlabled_train_comments.csv', index_col=0, sep=\"\\t\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iRZDgrhWVCi2"
   },
   "source": [
    "whole_comments = comments_labeled.append(comments_unlabeled)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lYFPiIsSW3e"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ex_9C2WBcuVU"
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTu2hL5L9lFn"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0aBzzqJDEMR"
   },
   "source": [
    "## Delete senseless comments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iiMRix54VIlx"
   },
   "source": [
    "whole_comments = whole_comments.fillna(\"Больше нечего сказать\")\n",
    "clean_whole_comments = whole_comments[[\"comment\"]][(whole_comments.comment != \"Больше нечего сказать\") & (whole_comments.comment != \"Да\")  & (whole_comments.comment != \"Нет\")]\n",
    "train_comments = train_labeled[[\"comment\", \"is_aggressive\"]][(train_labeled.comment != \"Больше нечего сказать\") & (train_labeled.comment != \"Да\")  & (train_labeled.comment != \"Нет\")]\n",
    "train_comments"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmUrMD__DIsC"
   },
   "source": [
    "## Text preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u-KO38D19OKO"
   },
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,\n",
    "                 punktuation: list,\n",
    "                 end_punktuation: list):\n",
    "\n",
    "        self.punkt = set(punktuation)\n",
    "        self.end_punkt = set(end_punktuation)\n",
    "        self.punkt = self.punkt  -  self.end_punkt\n",
    "\n",
    "    def remove_punkt(self, text: str) -> str:\n",
    "        for punc in self.punkt:\n",
    "            if punc in text:\n",
    "                text = text.replace(punc, ' ')\n",
    "        text = \" \".join(text.split())\n",
    "        return text.strip()\n",
    "\n",
    "    def replace_end_punkt(self, text: str) -> str:\n",
    "        for punc in self.end_punkt:\n",
    "            if punc in text:\n",
    "                text = text.replace(punc, '.')\n",
    "        split_text = text.split(\".\")\n",
    "        clean_text = [i for i in split_text if i != \"\"]\n",
    "        text = \" [SEP] \".join(clean_text)\n",
    "        return text\n",
    "\n",
    "    def preprocess(self, text: list) -> list:\n",
    "        no_punkt_text = list(map(self.remove_punkt, text))\n",
    "        sep_text = list(map(self.replace_end_punkt, no_punkt_text))\n",
    "        return sep_text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ih7NePmWQFA3"
   },
   "source": [
    "import string\n",
    "punktuation = list(string.punctuation)\n",
    "custom_punkt = [\"\\t\", \")\", \"(\"]\n",
    "punktuation.extend(custom_punkt)\n",
    "\n",
    "end_punktuation = [\"!\", \".\", \"?\", \"\\n\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nLfD1it1-QTF"
   },
   "source": [
    "preprocessor = Preprocessor(punktuation, end_punktuation)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ZeoHFqD9to2"
   },
   "source": [
    "list_comments = list(train_comments.comment)\n",
    "\n",
    "prep_comments = preprocessor.preprocess(list_comments)\n",
    "len(prep_comments)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hPbwSWAYuzJU"
   },
   "source": [
    "train_comments[\"comment\"] = prep_comments"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "16sdV41wu7aC"
   },
   "source": [
    "train_comments"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W4wxwBjnuzlb"
   },
   "source": [
    "train_comments = train_comments[train_comments.comment != \"\"]\n",
    "train_comments"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tklMUQh4Bt7i"
   },
   "source": [
    "train_comments.to_csv(\"Data/labeled_comments.csv\", index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqn_46Dvv9Dl"
   },
   "source": [
    "# Back translation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zDe5ovF1nFnR"
   },
   "source": [
    "import pandas as pd\n",
    "non_empty_comments = list(pd.read_csv(\"Data/ori_comments.csv\").comment)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpZnrglZVRRU"
   },
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "import sentencepiece\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0YNn9I6EVTto"
   },
   "source": [
    "target_model_name = 'Helsinki-NLP/opus-mt-ru-en'\n",
    "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
    "target_model = MarianMTModel.from_pretrained(target_model_name)\n",
    "target_model.to(device)\n",
    "\n",
    "en_model_name = 'Helsinki-NLP/opus-mt-en-ru'\n",
    "en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
    "en_model = MarianMTModel.from_pretrained(en_model_name)\n",
    "en_model.to(device)\n",
    "print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tbafng6WD7fA"
   },
   "source": [
    "def add_lang(text: str, lang: str) -> str:\n",
    "    if lang == \"en\":\n",
    "        return text\n",
    "    return f\">>{lang}<< {text}\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "euxoiVJwVYEq"
   },
   "source": [
    "def translate(texts, model, tokenizer, language):\n",
    "    clean_texts = []\n",
    "    for text in texts:\n",
    "        clean_text = text.replace(\"[SEP]\", \" \")\n",
    "        clean_texts.append(clean_text)\n",
    "        \n",
    "    encoded = tokenizer.prepare_seq2seq_batch(clean_texts)\n",
    "    encoded[\"input_ids\"] = torch.tensor(encoded[\"input_ids\"]).to(device)\n",
    "    encoded[\"attention_mask\"] = torch.tensor(encoded[\"attention_mask\"]).to(device)\n",
    "\n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**encoded)\n",
    "\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    \n",
    "    return translated_texts"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FqhpY6B5VaFw"
   },
   "source": [
    "def back_translate(texts, source_lang=\"ru\", target_lang=\"en\"):\n",
    "    # Translate from source to target language\n",
    "    \n",
    "    trans_texts = translate(texts, target_model, target_tokenizer,\n",
    "                         language=target_lang)\n",
    "\n",
    "    # Translate from target language back to source language\n",
    "    back_translated_texts = translate(trans_texts, en_model, en_tokenizer, \n",
    "                                      language=source_lang)\n",
    "    \n",
    "    return back_translated_texts"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WQc7aDL5oINs"
   },
   "source": [
    "print(len(non_empty_comments))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-UxaoFlZVc51"
   },
   "source": [
    "ori_texts = non_empty_comments\n",
    "aug_texts = []\n",
    "verbose = 100\n",
    "for i, text in enumerate(ori_texts):\n",
    "    aug_text = back_translate([text], source_lang=\"ru\", target_lang=\"en\")\n",
    "    aug_texts.append(aug_text)\n",
    "    if i % verbose == 0:\n",
    "        print(i)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZcJHeqCQHZbY"
   },
   "source": [
    "aug_text = pd.DataFrame()\n",
    "aug_text[\"comment\"] = aug_texts\n",
    "aug_text.to_csv(\"Data/aug_comments.csv\", index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XwJ-l4IcUZp9"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}